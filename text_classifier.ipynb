{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Sentiment analysis of IMDB movie reviews with various machine learning algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Exploratory data analysis\n",
    "We will now explore the data and see what we are working with. This is useful to help us see what type of cleaning we need to do.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Summary of the dataset\n",
    "Once we have created the dataframe by reading the csv file, we can use describe(), df.shape, and df.head to form an overview of the data. We can also generate a word cloud to get an overview of the most occurring words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                                   review sentiment\ncount                                               50000     50000\nunique                                              49581         2\ntop     love today show it wa a varieti and not sole c...  positive\nfreq                                                    5     25000",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>50000</td>\n      <td>50000</td>\n    </tr>\n    <tr>\n      <th>unique</th>\n      <td>49581</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>top</th>\n      <td>love today show it wa a varieti and not sole c...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>freq</th>\n      <td>5</td>\n      <td>25000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Description of the dataframe\n",
    "imdb_data.describe()\n",
    "#Lakshmipathi N 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can also print out the shape of our dataset to see the number of rows and columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 2)\n"
     ]
    }
   ],
   "source": [
    "# Shape of the dataframe (rows, columns)\n",
    "print(imdb_data.shape)\n",
    "#Lakshmipathi N 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "And we can use the head() function to see the first 10 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                              review sentiment\n0  One of the other reviewers has mentioned that ...  positive\n1  A wonderful little production. <br /><br />The...  positive\n2  I thought this was a wonderful way to spend ti...  positive\n3  Basically there's a family where a little boy ...  negative\n4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n5  Probably my all-time favorite movie, a story o...  positive\n6  I sure would like to see a resurrection of a u...  positive\n7  This show was an amazing, fresh & innovative i...  negative\n8  Encouraged by the positive comments about this...  negative\n9  If you like original gut wrenching laughter yo...  positive",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>One of the other reviewers has mentioned that ...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I thought this was a wonderful way to spend ti...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Basically there's a family where a little boy ...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Probably my all-time favorite movie, a story o...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>I sure would like to see a resurrection of a u...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>This show was an amazing, fresh &amp; innovative i...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Encouraged by the positive comments about this...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>If you like original gut wrenching laughter yo...</td>\n      <td>positive</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the top 10 rows of the dataset\n",
    "imdb_data.head(10)\n",
    "#Lakshmipathi N 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Note the \"</ br>\" text - this is probably something we want to remove. We can use a wordcloud to help us see some other potential problems too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Concatenate the text from the review column into a single string.\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m wordcloud_text \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(\u001B[43mdf\u001B[49m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mreview\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# Generate the wordcloud\u001B[39;00m\n\u001B[1;32m      5\u001B[0m wordcloud \u001B[38;5;241m=\u001B[39m WordCloud(width\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m800\u001B[39m, height\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m400\u001B[39m)\u001B[38;5;241m.\u001B[39mgenerate(wordcloud_text)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Concatenate the text from the review column into a single string.\n",
    "wordcloud_text = ' '.join(df['review'])\n",
    "\n",
    "# Generate the wordcloud\n",
    "wordcloud = WordCloud(width=800, height=400).generate(wordcloud_text)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Adapted from Valkov 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Looks good, other than the \"br\" which we'll have to deal with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Sentiment distribution\n",
    "We want to ensure that we have a balanced dataset to avoid introducing bias into our model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Draw a bar chart to show the distribution of the data in the sentiment column.\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m axis \u001B[38;5;241m=\u001B[39m \u001B[43mdf\u001B[49m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msentiment\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mvalue_counts()\u001B[38;5;241m.\u001B[39msort_index()\\\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;241m.\u001B[39mplot(kind\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbar\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m      4\u001B[0m           title\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDistribution of sentiment tags\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m      5\u001B[0m           figsize\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m5\u001B[39m,\u001B[38;5;241m5\u001B[39m))\n\u001B[1;32m      6\u001B[0m axis\u001B[38;5;241m.\u001B[39mset_xlabel(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mSentiments\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      7\u001B[0m plt\u001B[38;5;241m.\u001B[39mshow()\n",
      "\u001B[0;31mNameError\u001B[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Draw a bar chart to show the distribution of the data in the sentiment column.\n",
    "axis = df['sentiment'].value_counts().sort_index()\\\n",
    "    .plot(kind=\"bar\",\n",
    "          title=\"Distribution of sentiment tags\",\n",
    "          figsize=(5,5))\n",
    "axis.set_xlabel('Sentiments')\n",
    "plt.show()\n",
    "# New Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Looks like the dataset is balanced. Great!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Preprocessing\n",
    "In this section we'll clean up the data. We'll be following Lakshmipathi N's approach to preprocessing for now and then introduce some additional steps later.\n",
    "\n",
    "According to Valkov (2019) we should be aware that real world text data can be messy. When working with this dataset, they recommend removing HTML tags, punctuation and other special characters, and any excessive spaces.\n",
    "\n",
    "We'll create some functions to do this cleaning for us and then apply them to the IMDB data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Removing HTML and other junk text\n",
    "Removing HTML and other junk text like the \"<br>\" we noticed in the word cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#Function to strip HTML tags\n",
    "def strip_html(text):\n",
    "    # Check if the input looks like a file name\n",
    "    if os.path.isfile(text):\n",
    "        with open(text, 'r') as file:\n",
    "            soup = BeautifulSoup(file, \"html.parser\")\n",
    "    else:\n",
    "        soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "# Function to remove square brackets\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "\n",
    "# Function that combines the above to functions to clean up junk text\n",
    "def remove_junk_text(text):\n",
    "    text = strip_html(text)\n",
    "    text = remove_between_square_brackets(text)\n",
    "    return text\n",
    "\n",
    "# Apply the remove_junk_text function to the IMDB dataset\n",
    "imdb_data['review'] = imdb_data['review'].apply(remove_junk_text)\n",
    "\n",
    "#Adapted code from Lakshmipathi N 2020 with some changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Check if the \"<br>\" tags we saw before has been removed.\n",
    "imdb_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Removing special characters\n",
    "Removing punctuation and other characters that are not letters or digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Function to remove special characters\n",
    "def remove_special_characters(text, remove_digits=True):\n",
    "    pattern = r'[^a-zA-z0-9\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "# Apply the remove_special_characters function to the dataset review  column\n",
    "imdb_data['review'] = imdb_data['review'].apply(remove_special_characters)\n",
    "\n",
    "#Lakshmipathi N 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Removing excessive spaces\n",
    "Removing extra whitespace other than spaces between words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# The function to remove white spaces\n",
    "def remove_excessive_spaces(text):\n",
    "    cleaned_text = re.sub('\\s+', ' ', text)\n",
    "    return cleaned_text.strip()\n",
    "\n",
    "# Apply the function to the review column of the IMDB data\n",
    "imdb_data['review'] = imdb_data['review'].apply(remove_excessive_spaces)\n",
    "\n",
    "# New Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Text stemming\n",
    "Here we will be stemming the text using the Porter stemming algorithm and applying it to the 'review' column of the imdb_data dataframe.\n",
    "By applying stemming, the code reduces words to their base or root form, which can help in text analysis tasks such as text classification, information retrieval, or topic modeling. Stemming aims to normalize words and treat different variations of the same word as the same base word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#Stemming the text with the Porter stemming algorithm from NLTK\n",
    "def simple_stemmer(text):\n",
    "    ps=nltk.porter.PorterStemmer()\n",
    "    text= ' '.join([ps.stem(word) for word in text.split()])\n",
    "    return text\n",
    "#Apply function on review column\n",
    "imdb_data['review']=imdb_data['review'].apply(simple_stemmer)\n",
    "\n",
    "# Check the result of the stemming process\n",
    "imdb_data.head(5)\n",
    "\n",
    "#Lakshmipathi N 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Removing stopwords\n",
    "By removing stopwords we try to eliminate common words that often do not contribute significant meaning to the text analysis tasks. This can help improve the accuracy and efficiency of NLP applications such as sentiment analysis.\n",
    "\n",
    "First, we set the stopwords list to English, and then let's just check the list of words that will be excluded in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'but', 'our', 'between', 'why', 'up', \"doesn't\", 'when', 'there', 'what', 'have', 'which', 'are', 'only', 'him', 'further', \"wasn't\", 'just', 'under', 'for', \"should've\", 'in', 'herself', 'ain', 'then', 'whom', 'because', 'by', 'other', 'been', 'were', \"hasn't\", 'you', 'was', 'with', 'so', 'or', 'doing', 'her', 'we', 'all', 'is', 'any', 'can', 'those', 'am', 'nor', \"couldn't\", 'its', 'didn', 'should', 'his', 'did', 'i', 'me', 'again', 'as', \"it's\", 'this', 'of', 'no', \"needn't\", 'most', 'on', 'she', 'while', \"aren't\", 'few', 'own', 'more', 'about', 'against', 'shan', 'into', 'to', 'll', 'how', \"won't\", 'such', 'had', 'the', \"isn't\", \"you've\", 'both', 'very', 'where', 'their', 'themselves', 'ours', \"haven't\", 'my', 'over', \"wouldn't\", 'yourself', 'm', 'down', 'off', 'same', 'a', \"mightn't\", 'wasn', 'has', 'yourselves', 'ourselves', 'aren', 'wouldn', \"shan't\", 'they', 'hers', 'being', 's', 'haven', 'here', 'doesn', 'be', 'too', 'will', 'mustn', 'do', 'them', 'at', 'y', 'theirs', 'mightn', 'shouldn', 'an', 'himself', 'above', 'who', 'once', 'd', \"that'll\", 'weren', 'hasn', 'couldn', 'some', \"mustn't\", 'out', \"don't\", 'won', 'that', 'each', 'during', 'after', \"weren't\", 'now', 'before', 'itself', \"didn't\", 'if', \"shouldn't\", 'through', 'he', 'below', 'isn', 'not', \"hadn't\", 'it', 'yours', 'these', 'having', 'than', 'from', 'hadn', 't', \"she's\", 'myself', 've', 'and', \"you'd\", 'o', \"you'll\", 'don', 'your', \"you're\", 're', 'ma', 'until', 'does', 'needn'}\n"
     ]
    }
   ],
   "source": [
    "# Setting English stopwords\n",
    "stop=set(stopwords.words('english'))\n",
    "print(stop)\n",
    "\n",
    "#Lakshmipathi N 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example review stopwords .\n"
     ]
    }
   ],
   "source": [
    "#Initialise a tokenizer\n",
    "tokenizer=ToktokTokenizer()\n",
    "\n",
    "# Function to remove the stopwords\n",
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stop]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stop]\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text\n",
    "\n",
    "#Apply function on review column\n",
    "imdb_data['review']=imdb_data['review'].apply(remove_stopwords)\n",
    "\n",
    "#Lakshmipathi N 2020\n",
    "\n",
    "# Test an example\n",
    "review = \"This is an example review with stopwords.\"\n",
    "filtered_review = remove_stopwords(review)\n",
    "print(filtered_review)\n",
    "#New Code"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Feature extraction\n",
    "In this section we'll do some work to represent the text data numerically by extracting relevant features. We'll use the Bag-of-Words model, where each review is represented as a vector of word frequencies or presence/absence indicators. This will pair well with the Naive Bayes model we'll be using later.\n",
    "\n",
    "Let's use the CountVectorizer from the scikit-learn library to perform feature extraction with a bag-of-words approach."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words - cv_reviews: (50000, 7914506)\n"
     ]
    }
   ],
   "source": [
    "# Use the CountVectorizer for bag of words\n",
    "cv=CountVectorizer(min_df=0,max_df=1,binary=False,ngram_range=(1,3))\n",
    "\n",
    "# Transform reviews\n",
    "cv_reviews=cv.fit_transform(imdb_data.review)\n",
    "\n",
    "# Check results\n",
    "print('Bag of Words - cv_reviews:',cv_reviews.shape)\n",
    "\n",
    "#Adapted code from Lakshmipathi N 2020 with some changes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We also need to convert the sentiment data into binary form. This is because when optimizing for sentiment analysis, occurrence of a word matters more than the frequency of the word.\n",
    "\n",
    "\n",
    "Let's use LabelBinarizer from scikit-learn to give each sentiment value (which is \"positive\" and \"negative\" at the moment) a binary value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 1)\n"
     ]
    }
   ],
   "source": [
    "# Label the sentiment data\n",
    "lb=LabelBinarizer()\n",
    "\n",
    "# Transform the sentiment data\n",
    "sentiment_data=lb.fit_transform(imdb_data.sentiment)\n",
    "\n",
    "# Check the results\n",
    "print(sentiment_data.shape)\n",
    "\n",
    "#Adapted code from Lakshmipathi N 2020 with some changes"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Testing and training data split\n",
    "It's common to split a dataset into two separate sets to use for training and testing. As we can infer from the names, one set, usually larger, is used to train a model, while the other set is used to test the model on to assess the performance."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "While it seems to work, the approach to splitting testing and training data used by Lakshmipathi N 2020 is somewhat cumbersome.\n",
    "\n",
    "Let's use the train_test_split function from SKlearn to do this for us instead."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# Use sklearn to split the dataset into training and testing datasets\n",
    "x = cv_reviews #input features\n",
    "y = sentiment_data #labels\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1)\n",
    "\n",
    "#New Code"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Classification approach"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this section we'll fit the data to a Naive Bayes model. We'll compare the results of this model with the baseline we got from the Lakshmipathi N notebook. We discussed this in the previous section.\n",
    "\n",
    "#### Multinomial Naive Bayes classification\n",
    "* Features are generated from a multinomial distribution by observing counts across categories.\n",
    "* Data must be converted from strings to numbers. We can use TfidfVectorizer, or a bag-of-words approach with the CountVectorizer like we did earlier.\n",
    "\n",
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB()\n"
     ]
    }
   ],
   "source": [
    "# Train the multinomial Naive Bayes model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import numpy as np\n",
    "\n",
    "mnb_model = MultinomialNB()\n",
    "\n",
    "# Reshape train_sentiments into a 1-dimensional array\n",
    "y_train_1d = np.ravel(y_train)\n",
    "y_test_1d = np.ravel(y_test)\n",
    "\n",
    "# Fit the bag of words data using the training reviews and 1-dimensional train_sentiments\n",
    "mnb_model_bagofwords = mnb_model.fit(x_train, y_train_1d)\n",
    "print(mnb_model_bagofwords)\n",
    "\n",
    "#Adapted code from Lakshmipathi N 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Using the model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Predict the model\n",
    "mnb_model_bagofwords_predict = mnb_model_bagofwords.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnb_model_bagofwords_score : 0.4954\n"
     ]
    }
   ],
   "source": [
    "#Accuracy score for bag of words\n",
    "\n",
    "mnb_model_bagofwords_score = accuracy_score(y_test_1d,mnb_model_bagofwords_predict)\n",
    "print(\"mnb_model_bagofwords_score :\",mnb_model_bagofwords_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Classification report for benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.50      0.97      0.66      5044\n",
      "    Negative       0.23      0.01      0.01      4956\n",
      "\n",
      "    accuracy                           0.50     10000\n",
      "   macro avg       0.36      0.49      0.34     10000\n",
      "weighted avg       0.36      0.50      0.34     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compile a classification report\n",
    "from sklearn.metrics import classification_report\n",
    "mnb_model_bagofwords_report = classification_report(y_test_1d, mnb_model_bagofwords_predict, target_names = ['Positive', 'Negative'])\n",
    "print(mnb_model_bagofwords_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "So for our first round, we got a accuracy score of 0.4954, which is much lower than the baseline of 0.751."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Iterating\n",
    "Lets make some changes and see if they have an impact on our performance.\n",
    "\n",
    "For the MNB model, the preprocessing techniques that was used for the baseline was:\n",
    "* Removed HTML tags\n",
    "* Remove special characters\n",
    "* Remove spaces\n",
    "* Stem the text\n",
    "* Remove stopwords\n",
    "\n",
    "Tinkering with the data\n",
    "Let's rewrite all of the processing functions from before into a single function which does the following:\n",
    "* Remove HTML tags and junk text\n",
    "* Remove extra spaces\n",
    "* Convert all words to lowercase (so ‘good’ and ‘Good’ aren’t counted is two different words)\n",
    "* Keep special characters like punctuation to make handling negation easier\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "#New dataframe\n",
    "import pandas as pd\n",
    "movie_data = pd.read_csv('IMDB_dataset.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "#Preprocessing function\n",
    "\n",
    "def preprocess(text):\n",
    "    # Check if the input looks like a file name\n",
    "    if os.path.isfile(text):\n",
    "        with open(text, 'r') as file:\n",
    "            soup = BeautifulSoup(file, \"html.parser\")\n",
    "    else:\n",
    "        soup = BeautifulSoup(text, \"html.parser\")\n",
    "\n",
    "    cleaned_text = soup.get_text()\n",
    "\n",
    "    # Replace [] with nothing\n",
    "    cleaned_text = re.sub(r'\\[[^]]*\\]', '', cleaned_text)\n",
    "\n",
    "    # Remove excessive white spaces\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "\n",
    "    # Convert to lowercase\n",
    "    cleaned_text = cleaned_text.lower()\n",
    "\n",
    "    # Preserve negation\n",
    "    cleaned_text = re.sub(r'\\b(n\\'t|not|no|never)\\s+(\\w+)', r'not_\\1 \\2', cleaned_text)\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "#Adapted code from Lakshmipathi N 2020 with substantial changes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fp/sfmgsyr555b7vfy1lp18nrjw0000gn/T/ipykernel_65281/3295716233.py:9: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, \"html.parser\")\n"
     ]
    }
   ],
   "source": [
    "# Apply preprossing to the dataframe\n",
    "movie_data['review'] = movie_data['review'].apply(preprocess)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "In the preprocessing function above, we have avoided removing punctuation and also tried to preserve negation. There are some further things we will do differently:\n",
    "* Avoid stemming the words\n",
    "* Avoid removing stopwords (According to Jurafsky and Martin (2023) removing stopwords doesn’t improve the performance of text classifications applications.)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Initialize the CountVectorizer\n",
    "vectorizer = CountVectorizer(min_df=0,max_df=1,binary=False,ngram_range=(1,3))\n",
    "\n",
    "# Fit the vectorizer on the preprocessed text data\n",
    "X = vectorizer.fit_transform(movie_data.review)\n",
    "\n",
    "# Convert the feature matrix to an array\n",
    "X = X.toarray()\n",
    "\n",
    "# Get the feature names from the vocabulary\n",
    "feature_names = vectorizer.get_feature_names()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Print the shape of the feature matrix\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mShape of feature matrix:\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[43mX\u001B[49m\u001B[38;5;241m.\u001B[39mshape)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "# Print the shape of the feature matrix\n",
    "print(\"Shape of feature matrix:\", X.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## III Conclusions\n",
    "tbc"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}